# Inference provider type: choose from "hf" | "api" | "vllm"
provider: xxx   # e.g., "hf" (HuggingFace), "api" (remote API), "vllm" (local vLLM)

# Model name or path:
# - For provider=hf or vllm: local path or HuggingFace Hub name
# - For provider=api: API model name
model: xxxxxx

# Common generation parameters (shared across all providers; some may be ignored depending on provider)
sampling:
  temperature: 0.7         # Sampling temperature for diversity
  max_tokens: 8192         # Maximum number of tokens to generate
  top_p: 0.95              # Nucleus sampling (top-p) threshold
  top_k: 50                # Top-k sampling limit
  system_prompt: "You need to put the final result inside <answer></answer>."  # System-level prompt, if supported

# HuggingFace pipeline parameters (only used when provider=hf)
hf:
  trust_remote_code: true  # Allow loading custom code from remote repositories

# vLLM-specific parameters (only used when provider=vllm)
vllm:
  max_model_len: 10000     # Maximum context length supported by the model (default is 10000)

# API-specific parameters (only used when provider=api)
api:
  base_url: https://your-api-base-url.com/v1  # API endpoint URL
  api_key: your-api-key                       # API key (keep secure)
  threads: 50                                  # Number of concurrent threads
